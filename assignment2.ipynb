{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization for Exploring the European Parliament's Topic Agenda\n",
    "### Assignment 2 for Machine Learning Complements class\n",
    "By Alexandra de Carvalho, LuÃ­s Costa, Nuno Pedrosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the needed Python libraries\n",
    "We will use Pandas for dataframe manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alexa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alexa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/alexa/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alexa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# for modeling \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# for text processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand pandas df column display width to enable easy inspection\n",
    "pd.set_option('max_colwidth', 150)\n",
    "\n",
    "# read the textfiles to a dataframe\n",
    "dir_path = 'sample' # folder path\n",
    "files = [] # list to store files\n",
    "\n",
    "for path in os.listdir(dir_path):\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        files.append(os.path.join(dir_path, path))\n",
    "    else:\n",
    "        subpath = os.path.join(dir_path, path)\n",
    "        for path2 in os.listdir(subpath):\n",
    "            if os.path.isfile(os.path.join(subpath, path2)):\n",
    "                files.append(os.path.join(subpath, path2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "To make all of the text in the speeches as comparable as possible we need to remove punctuation, capitalization, numbers, and strange characters. We also keep the term frequency on each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = dict()\n",
    "for filename in files:\n",
    "    with open(filename, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        text_tokens[filename] = dict()\n",
    "        \n",
    "        for line in lines:\n",
    "            for token in re.split('\\W+', str(line)):\n",
    "                token = token.lower()\n",
    "                if len(token) > 3 and not token.isnumeric() and not token.lower() in stopwords.words('english'):\n",
    "                    text_tokens[filename][token] = text_tokens[filename].get(token, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()   # stored function to lemmatize each word\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "nouns = dict()\n",
    "for filename, tokens in text_tokens.items():\n",
    "    if filename not in nouns:\n",
    "        nouns[filename] = dict()\n",
    "\n",
    "    for (word, pos) in pos_tag(list(tokens.keys())):\n",
    "        if is_noun(pos):\n",
    "            nouns[filename][wordnet_lemmatizer.lemmatize(word)] = nouns[filename].get(wordnet_lemmatizer.lemmatize(word), 0) + text_tokens[filename][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the matrix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, only with the term frequency weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictvectorizer = DictVectorizer(sparse=False)\n",
    "a = dictvectorizer.fit_transform(list(nouns.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the list of all tokens (all columns of A, in order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = dictvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculating updating to TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_idx in range(len(token_list)):\n",
    "    idf = math.log(len(a[:, column_idx])/len([x for x in a[:, column_idx] if x != 0]), 10)\n",
    "\n",
    "    for element_idx in range(len(files)):\n",
    "        if a[element_idx,column_idx] != 0:\n",
    "            a[element_idx,column_idx] = (math.log(a[element_idx,column_idx], 10) + 1) * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO : USE W2V TO FIND BEST K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(k) \n",
    "w = nmf_model.fit_transform(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO : VISUALISING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in enumerate(nmf_model.components_):\n",
    "    \n",
    "    print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "    print(\", \".join([feature_names[i] \\\n",
    "            for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
