{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization for Exploring the European Parliament's Topic Agenda\n",
    "### Assignment 2 for Machine Learning Complements class\n",
    "By Alexandra de Carvalho, LuÃ­s Costa, Nuno Pedrosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the needed Python libraries\n",
    "We will use Pandas for dataframe manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/luismiguel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/luismiguel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/luismiguel/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/luismiguel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# for modeling \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# for text processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand pandas df column display width to enable easy inspection\n",
    "pd.set_option('max_colwidth', 150)\n",
    "\n",
    "# read the textfiles to a dataframe\n",
    "dir_path = 'sample' # folder path\n",
    "files = [] # list to store files\n",
    "\n",
    "for path in os.listdir(dir_path):\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        files.append(os.path.join(dir_path, path))\n",
    "    else:\n",
    "        subpath = os.path.join(dir_path, path)\n",
    "        for path2 in os.listdir(subpath):\n",
    "            if os.path.isfile(os.path.join(subpath, path2)):\n",
    "                files.append(os.path.join(subpath, path2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "To make all of the text in the speeches as comparable as possible we need to remove punctuation, capitalization, numbers, and strange characters. We also keep the term frequency on each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = dict()\n",
    "for filename in files:\n",
    "    with open(filename, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        text_tokens[filename] = dict()\n",
    "        \n",
    "        for line in lines:\n",
    "            for token in re.split('\\W+', str(line)):\n",
    "                token = token.lower()\n",
    "                if len(token) > 3 and not token.isnumeric() and not token.lower() in stopwords.words('english'):\n",
    "                    text_tokens[filename][token] = text_tokens[filename].get(token, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important step is to lemmatize the words, that is to convert verbs and adjetives into their base form. This allows us to analyse related words as a single one and reduces the number of words in the documents matrix, reducing sparcity.\n",
    "\n",
    "Ex: walked -> walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()   # stored function to lemmatize each word\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "nouns = dict()\n",
    "for filename, tokens in text_tokens.items():\n",
    "    if filename not in nouns:\n",
    "        nouns[filename] = dict()\n",
    "\n",
    "    for (word, pos) in pos_tag(list(tokens.keys())):\n",
    "        if is_noun(pos):\n",
    "            nouns[filename][wordnet_lemmatizer.lemmatize(word)] = nouns[filename].get(wordnet_lemmatizer.lemmatize(word), 0) + text_tokens[filename][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the matrix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, only with the term frequency weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictvectorizer = DictVectorizer(sparse=False)\n",
    "a = dictvectorizer.fit_transform(list(nouns.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the list of all tokens (all columns of A, in order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "token_list = dictvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculating updating to TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_idx in range(len(token_list)):\n",
    "    idf = math.log(len(a[:, column_idx])/len([x for x in a[:, column_idx] if x != 0]), 10)\n",
    "\n",
    "    for element_idx in range(len(files)):\n",
    "        if a[element_idx,column_idx] != 0:\n",
    "            a[element_idx,column_idx] = (math.log(a[element_idx,column_idx], 10) + 1) * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best value for K : TC-W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the same number of terms per topic *t* as the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of terms per topic\n",
    "t = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the NMF with a number of topics that ranges between 10 and 26 in order to find the ammount of topics that gives us a model with the biggest coherence. The coherence of each topic is measured using the cosine similarity. The coherence of a model is the mean coherence of the topics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  10 . Model coherence: 0.0010334041642232083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  11 . Model coherence: 0.0011395746625658836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  12 . Model coherence: 0.003293048427440226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  13 . Model coherence: -0.001196117940334937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  14 . Model coherence: 0.003471767074531979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  15 . Model coherence: 0.0023252782011749574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  16 . Model coherence: 0.0024432338787139284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  17 . Model coherence: 0.0025310572151557302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  18 . Model coherence: 0.004904916359939509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  19 . Model coherence: 0.003393829649325177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  20 . Model coherence: 0.0034462484406928215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  21 . Model coherence: 0.0022019254566027374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  22 . Model coherence: 0.005777315026374927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  23 . Model coherence: 0.002602588914442753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  24 . Model coherence: 0.0034506020235346145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  25 . Model coherence: -0.0014814236255155672\n"
     ]
    }
   ],
   "source": [
    "max_model_coherence = 0\n",
    "res_k = 0\n",
    "\n",
    "for k in range(10,26):\n",
    "\n",
    "    nmf_model = NMF(k) \n",
    "    nmf_model.fit_transform(a)\n",
    "\n",
    "    vocabulary = [[token_list[x[1]] for x in sorted(zip(topic,range(len(topic))), reverse = True)[:t]] for topic in nmf_model.components_]\n",
    "    model = Word2Vec(sentences = vocabulary, vector_size = 200, window = 5, hs = 1, negative = 0, min_count = 1)\n",
    "    \n",
    "    # calculating individual topic coherence scores for each topic\n",
    "    model_score = []\n",
    "    for topic in vocabulary:\n",
    "        topic_score = []\n",
    "        for w1 in topic:\n",
    "            for w2 in topic:\n",
    "                if w2 > w1:\n",
    "                    word_score = cosine_similarity(model.wv[w2].reshape(1,-1),model.wv[w1].reshape(1,-1))[0]\n",
    "                    topic_score.append(word_score[0])\n",
    "        \n",
    "        topic_score = sum(topic_score)/len(topic_score) # mean of each word pair similarity in the topic\n",
    "        model_score.append(topic_score)\n",
    "\n",
    "    model_coherence = sum(model_score)/len(model_score) # mean of topic coherence in the model\n",
    "    print(\"k = \",k, \". Model coherence:\", model_coherence)\n",
    "\n",
    "    # used in order to choose the number of topics that has the biggest coherence\n",
    "    if model_coherence > max_model_coherence:\n",
    "        max_model_coherence = model_coherence\n",
    "        res_k = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of topics that gives us the biggest coherence is 22, so that is the number of topics that we will use in our definitive NMF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(res_k) \n",
    "w = nmf_model.fit_transform(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, find the t higher weights' index and find the correpondent token (same index) in the token list. These are the descriptors of each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : ['government', 'education', 'minister', 'health', 'child', 'school', 'department', 'council', 'proposal', 'report']\n",
      "Topic 1 : ['player', 'club', 'game', 'football', 'team', 'season', 'chelsea', 'manager', 'champion', 'match']\n",
      "Topic 2 : ['lord', 'court', 'right', 'law', 'trial', 'case', 'detainee', 'police', 'lawyer', 'detention']\n",
      "Topic 3 : ['music', 'band', 'song', 'rock', 'singer', 'artist', 'album', 'single', 'record', 'chart']\n",
      "Topic 4 : ['forsyth', 'frederick', 'terrorist', 'internment', 'forsythe', 'totalitarianism', 'qaeda', 'fundamentalism', 'churchill', 'liberty']\n",
      "Topic 5 : ['growth', 'economy', 'market', 'price', 'rate', 'rise', 'bank', 'analyst', 'dollar', 'profit']\n",
      "Topic 6 : ['angel', 'rhapsody', 'bland', 'guy', 'pulp', 'cheesy', 'brit', 'scissor', 'joke', 'listener']\n",
      "Topic 7 : ['sub', 'ball', 'minute', 'cech', 'duff', 'goal', 'header', 'kezman', 'robben', 'shot']\n",
      "Topic 8 : ['virus', 'program', 'mail', 'security', 'software', 'computer', 'attack', 'user', 'machine', 'firm']\n",
      "Topic 9 : ['yukos', 'bankruptcy', 'gazprom', 'russia', 'rosneft', 'khodorkovsky', 'court', 'unit', 'yugansk', 'fraud']\n",
      "Topic 10 : ['film', 'actor', 'award', 'oscar', 'actress', 'star', 'nomination', 'comedy', 'aviator', 'ceremony']\n",
      "Topic 11 : ['phone', 'network', 'operator', 'service', 'handset', 'speed', 'technology', 'customer', 'mobile', 'broadband']\n",
      "Topic 12 : ['blair', 'chancellor', 'peston', 'election', 'minister', 'book', 'ally', 'speculation', 'journalist', 'prescott']\n",
      "Topic 13 : ['parry', 'gerrard', 'stadium', 'xa350m', 'rafa', 'tycoon', 'sponsorship', 'club', 'benitez', 'deal']\n",
      "Topic 14 : ['chip', 'computer', 'silicon', 'science', 'researcher', 'technology', 'intel', 'supercomputer', 'gene', 'engineering']\n",
      "Topic 15 : ['gadget', 'sinclair', 'modern', 'marine', 'chronometer', 'calculator', 'notebook', 'apple', 'walkman', 'startac']\n",
      "Topic 16 : ['search', 'site', 'google', 'engine', 'information', 'user', 'blog', 'jeeves', 'website', 'internet']\n",
      "Topic 17 : ['joss', 'stone', 'soul', 'lemar', 'jazz', 'originate', 'genre', 'meaningless', 'brit', 'therefore']\n",
      "Topic 18 : ['definition', 'dvd', 'format', 'disc', 'picture', 'hollywood', 'technology', 'film', 'studio', 'resolution']\n",
      "Topic 19 : ['developer', 'game', 'xbox', 'console', 'gamers', 'title', 'playstation', 'video', 'halo', 'graphic']\n",
      "Topic 20 : ['toure', 'henry', 'bergkamp', 'vieira', 'almunia', 'fabregas', 'kenny', 'jagielka', 'wenger', 'flamini']\n",
      "Topic 21 : ['election', 'party', 'tory', 'howard', 'democrat', 'immigration', 'blair', 'leader', 'asylum', 'conservative']\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(nmf_model.components_):\n",
    "    print(\"Topic\", i, \":\",[token_list[x[1]] for x in sorted(zip(topic,range(len(topic))), reverse = True)[:t]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then observe the documents with bigger weights for each topic. Because the files names already tag the contained speech by topic, we can infer the validity of the model built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : ['entertainment_131.txt', 'politics_080.txt', 'politics_244.txt', 'politics_055.txt', 'politics_056.txt', 'politics_246.txt', 'business241.txt', 'politics_020.txt', 'tech_399.txt', 'politics_127.txt']\n",
      "Topic 1 : ['football_207.txt', 'football_087.txt', 'football_088.txt', 'football_086.txt', 'football_223.txt', 'football_202.txt', 'football_085.txt', 'football_221.txt', 'football_141.txt', 'football_124.txt']\n",
      "Topic 2 : ['politics_224.txt', 'politics_040.txt', 'politics_221.txt', 'politics_137.txt', 'politics_192.txt', 'politics_160.txt', 'politics_205.txt', 'politics_168.txt', 'politics_066.txt', 'politics_134.txt']\n",
      "Topic 3 : ['entertainment_262.txt', 'entertainment_229.txt', 'entertainment_244.txt', 'entertainment_263.txt', 'entertainment_142.txt', 'entertainment_131.txt', 'entertainment_133.txt', 'entertainment_145.txt', 'entertainment_236.txt', 'entertainment_153.txt']\n",
      "Topic 4 : ['politics_290.txt', 'politics_052.txt', 'politics_160.txt', 'politics_263.txt', 'politics_189.txt', 'politics_155.txt', 'tech_245.txt', 'entertainment_057.txt', 'politics_280.txt', 'politics_027.txt']\n",
      "Topic 5 : ['business290.txt', 'business262.txt', 'business119.txt', 'business163.txt', 'business044.txt', 'business245.txt', 'business286.txt', 'business126.txt', 'business066.txt', 'business272.txt']\n",
      "Topic 6 : ['entertainment_253.txt', 'entertainment_251.txt', 'entertainment_201.txt', 'entertainment_128.txt', 'entertainment_013.txt', 'football_198.txt', 'entertainment_131.txt', 'entertainment_215.txt', 'entertainment_191.txt', 'entertainment_106.txt']\n",
      "Topic 7 : ['football_010.txt', 'football_253.txt', 'football_246.txt', 'football_045.txt', 'football_184.txt', 'football_152.txt', 'football_011.txt', 'football_007.txt', 'football_247.txt', 'football_012.txt']\n",
      "Topic 8 : ['tech_270.txt', 'tech_299.txt', 'tech_077.txt', 'tech_281.txt', 'tech_398.txt', 'tech_227.txt', 'tech_308.txt', 'tech_083.txt', 'tech_210.txt', 'tech_026.txt']\n",
      "Topic 9 : ['business192.txt', 'business083.txt', 'business025.txt', 'business299.txt', 'business164.txt', 'business127.txt', 'business230.txt', 'business003.txt', 'business077.txt', 'business181.txt']\n",
      "Topic 10 : ['entertainment_095.txt', 'entertainment_064.txt', 'entertainment_038.txt', 'entertainment_275.txt', 'entertainment_039.txt', 'entertainment_082.txt', 'entertainment_069.txt', 'entertainment_051.txt', 'entertainment_078.txt', 'entertainment_086.txt']\n",
      "Topic 11 : ['tech_032.txt', 'tech_335.txt', 'tech_389.txt', 'tech_234.txt', 'tech_388.txt', 'tech_103.txt', 'tech_212.txt', 'tech_225.txt', 'tech_378.txt', 'tech_349.txt']\n",
      "Topic 12 : ['politics_253.txt', 'politics_218.txt', 'politics_255.txt', 'politics_252.txt', 'politics_256.txt', 'politics_260.txt', 'politics_254.txt', 'politics_225.txt', 'politics_257.txt', 'politics_251.txt']\n",
      "Topic 13 : ['football_018.txt', 'football_027.txt', 'football_024.txt', 'business242.txt', 'football_149.txt', 'business229.txt', 'business209.txt', 'football_180.txt', 'business224.txt', 'football_157.txt']\n",
      "Topic 14 : ['tech_161.txt', 'tech_173.txt', 'tech_019.txt', 'tech_329.txt', 'tech_137.txt', 'tech_141.txt', 'tech_320.txt', 'tech_165.txt', 'tech_166.txt', 'tech_018.txt']\n",
      "Topic 15 : ['tech_009.txt', 'tech_208.txt', 'tech_367.txt', 'tech_341.txt', 'tech_045.txt', 'tech_375.txt', 'tech_111.txt', 'tech_216.txt', 'tech_324.txt', 'tech_044.txt']\n",
      "Topic 16 : ['tech_213.txt', 'tech_010.txt', 'tech_016.txt', 'tech_318.txt', 'tech_062.txt', 'tech_053.txt', 'tech_381.txt', 'tech_066.txt', 'tech_337.txt', 'tech_261.txt']\n",
      "Topic 17 : ['entertainment_256.txt', 'tech_005.txt', 'entertainment_264.txt', 'entertainment_251.txt', 'entertainment_263.txt', 'entertainment_249.txt', 'tech_310.txt', 'tech_089.txt', 'entertainment_142.txt', 'entertainment_229.txt']\n",
      "Topic 18 : ['tech_294.txt', 'tech_155.txt', 'tech_313.txt', 'tech_094.txt', 'tech_306.txt', 'tech_224.txt', 'tech_219.txt', 'tech_360.txt', 'tech_216.txt', 'tech_240.txt']\n",
      "Topic 19 : ['tech_396.txt', 'tech_135.txt', 'tech_309.txt', 'tech_139.txt', 'tech_154.txt', 'tech_125.txt', 'tech_052.txt', 'tech_178.txt', 'tech_130.txt', 'tech_244.txt']\n",
      "Topic 20 : ['football_260.txt', 'football_069.txt', 'football_174.txt', 'football_259.txt', 'football_255.txt', 'football_133.txt', 'football_152.txt', 'football_084.txt', 'football_215.txt', 'football_198.txt']\n",
      "Topic 21 : ['politics_283.txt', 'politics_272.txt', 'politics_159.txt', 'politics_284.txt', 'politics_147.txt', 'politics_087.txt', 'politics_220.txt', 'politics_275.txt', 'politics_022.txt', 'politics_213.txt']\n"
     ]
    }
   ],
   "source": [
    "for i in range(res_k):\n",
    "    print(\"Topic\", i, \":\",[files[x[1]].split('/')[-1] for x in sorted(zip(w[:,i],range(len(w[:,i]))), reverse = True)[:t]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, theres is definitively a match between the terms in each topic and the name of the speechs from which they were recovered. This results are aligned with what was expected and as such verify that the model built is a valid one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f77e6bc2d225f99816d788c5a4a60bbea5b0f9a625286da74699d4a3f8b02a8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
