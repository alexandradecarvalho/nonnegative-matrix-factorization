{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization for Exploring the European Parliament's Topic Agenda\n",
    "### Assignment 2 for Machine Learning Complements class\n",
    "By Alexandra de Carvalho, LuÃ­s Costa, Nuno Pedrosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the needed Python libraries\n",
    "We will use Pandas for dataframe manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alexa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alexa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/alexa/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alexa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# for modeling \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# for text processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand pandas df column display width to enable easy inspection\n",
    "pd.set_option('max_colwidth', 150)\n",
    "\n",
    "# read the textfiles to a dataframe\n",
    "dir_path = 'sample' # folder path\n",
    "files = [] # list to store files\n",
    "\n",
    "for path in os.listdir(dir_path):\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        files.append(os.path.join(dir_path, path))\n",
    "    else:\n",
    "        subpath = os.path.join(dir_path, path)\n",
    "        for path2 in os.listdir(subpath):\n",
    "            if os.path.isfile(os.path.join(subpath, path2)):\n",
    "                files.append(os.path.join(subpath, path2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "To make all of the text in the speeches as comparable as possible we need to remove punctuation, capitalization, numbers, and strange characters. We also keep the term frequency on each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = dict()\n",
    "for filename in files:\n",
    "    with open(filename, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        text_tokens[filename] = dict()\n",
    "        \n",
    "        for line in lines:\n",
    "            for token in re.split('\\W+', str(line)):\n",
    "                token = token.lower()\n",
    "                if len(token) > 3 and not token.isnumeric() and not token.lower() in stopwords.words('english'):\n",
    "                    text_tokens[filename][token] = text_tokens[filename].get(token, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()   # stored function to lemmatize each word\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "nouns = dict()\n",
    "for filename, tokens in text_tokens.items():\n",
    "    if filename not in nouns:\n",
    "        nouns[filename] = dict()\n",
    "\n",
    "    for (word, pos) in pos_tag(list(tokens.keys())):\n",
    "        if is_noun(pos):\n",
    "            nouns[filename][wordnet_lemmatizer.lemmatize(word)] = nouns[filename].get(wordnet_lemmatizer.lemmatize(word), 0) + text_tokens[filename][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the matrix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, only with the term frequency weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictvectorizer = DictVectorizer(sparse=False)\n",
    "a = dictvectorizer.fit_transform(list(nouns.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the list of all tokens (all columns of A, in order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexa/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "token_list = dictvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculating updating to TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_idx in range(len(token_list)):\n",
    "    idf = math.log(len(a[:, column_idx])/len([x for x in a[:, column_idx] if x != 0]), 10)\n",
    "\n",
    "    for element_idx in range(len(files)):\n",
    "        if a[element_idx,column_idx] != 0:\n",
    "            a[element_idx,column_idx] = (math.log(a[element_idx,column_idx], 10) + 1) * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO : USE W2V TO FIND BEST K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(k) \n",
    "w = nmf_model.fit_transform(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO : VISUALISING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, find the t higher weights' index and find the correpondent token (same index) in the token list. These are the descriptors of each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : ['government', 'people', 'service', 'minister', 'system', 'technology', 'firm', 'company', 'country', 'plan']\n",
      "Topic 1 : ['game', 'player', 'goal', 'chelsea', 'club', 'team', 'football', 'manager', 'minute', 'chance']\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(nmf_model.components_):\n",
    "    print(\"Topic\", i, \":\",[token_list[x[1]] for x in sorted(zip(topic,range(len(topic))), reverse = True)[:t]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36705923, 0.01384133],\n",
       "       [0.26643491, 0.70987538],\n",
       "       [0.2245102 , 0.        ],\n",
       "       ...,\n",
       "       [0.22599703, 0.01654774],\n",
       "       [0.09393832, 0.13258903],\n",
       "       [0.03249447, 0.1660655 ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : ['politics_290.txt', 'tech_399.txt', 'business245.txt', 'tech_164.txt', 'business277.txt', 'tech_032.txt', 'tech_335.txt', 'business287.txt', 'tech_199.txt', 'business146.txt']\n",
      "Topic 1 : ['entertainment_253.txt', 'football_207.txt', 'entertainment_256.txt', 'football_010.txt', 'football_152.txt', 'football_045.txt', 'football_253.txt', 'football_246.txt', 'football_007.txt', 'football_174.txt']\n"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    print(\"Topic\", i, \":\",[files[x[1]].split('/')[-1] for x in sorted(zip(w[:,i],range(len(w[:,i]))), reverse = True)[:t]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
