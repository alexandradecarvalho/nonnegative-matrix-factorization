{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization for Exploring the European Parliament's Topic Agenda\n",
    "### Assignment 2 for Machine Learning Complements class\n",
    "By Alexandra de Carvalho, LuÃ­s Costa, Nuno Pedrosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the needed Python libraries\n",
    "We will use Pandas for dataframe manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alexa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alexa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/alexa/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alexa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# for modeling \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# for text processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand pandas df column display width to enable easy inspection\n",
    "pd.set_option('max_colwidth', 150)\n",
    "\n",
    "# read the textfiles to a dataframe\n",
    "dir_path = 'sample' # folder path\n",
    "files = [] # list to store files\n",
    "\n",
    "for path in os.listdir(dir_path):\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        files.append(os.path.join(dir_path, path))\n",
    "    else:\n",
    "        subpath = os.path.join(dir_path, path)\n",
    "        for path2 in os.listdir(subpath):\n",
    "            if os.path.isfile(os.path.join(subpath, path2)):\n",
    "                files.append(os.path.join(subpath, path2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "To make all of the text in the speeches as comparable as possible we need to remove punctuation, capitalization, numbers, and strange characters. We also keep the term frequency on each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = dict()\n",
    "for filename in files:\n",
    "    with open(filename, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        text_tokens[filename] = dict()\n",
    "        \n",
    "        for line in lines:\n",
    "            for token in re.split('\\W+', str(line)):\n",
    "                token = token.lower()\n",
    "                if len(token) > 3 and not token.isnumeric() and not token.lower() in stopwords.words('english'):\n",
    "                    text_tokens[filename][token] = text_tokens[filename].get(token, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()   # stored function to lemmatize each word\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "nouns = dict()\n",
    "for filename, tokens in text_tokens.items():\n",
    "    if filename not in nouns:\n",
    "        nouns[filename] = dict()\n",
    "\n",
    "    for (word, pos) in pos_tag(list(tokens.keys())):\n",
    "        if is_noun(pos):\n",
    "            nouns[filename][wordnet_lemmatizer.lemmatize(word)] = nouns[filename].get(wordnet_lemmatizer.lemmatize(word), 0) + text_tokens[filename][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the matrix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, only with the term frequency weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictvectorizer = DictVectorizer(sparse=False)\n",
    "a = dictvectorizer.fit_transform(list(nouns.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the list of all tokens (all columns of A, in order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexa/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "token_list = dictvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculating updating to TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_idx in range(len(token_list)):\n",
    "    idf = math.log(len(a[:, column_idx])/len([x for x in a[:, column_idx] if x != 0]), 10)\n",
    "\n",
    "    for element_idx in range(len(files)):\n",
    "        if a[element_idx,column_idx] != 0:\n",
    "            a[element_idx,column_idx] = (math.log(a[element_idx,column_idx], 10) + 1) * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : normalize the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best value for K : TC-W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the same t as the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  10 . Model coherence: 0.0023823076238234835\n",
      "k =  11 . Model coherence: 0.0023658915922913533\n",
      "k =  12 . Model coherence: 0.0019045398576525733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexa/.local/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  13 . Model coherence: -0.0012079225394389251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexa/.local/lib/python3.10/site-packages/sklearn/decomposition/_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  14 . Model coherence: 0.0007569255819636587\n",
      "k =  15 . Model coherence: 0.00217567439557536\n",
      "k =  16 . Model coherence: -0.00023898034823004177\n",
      "k =  17 . Model coherence: 0.0030269980628432986\n",
      "k =  18 . Model coherence: 0.0029971630723863462\n",
      "k =  19 . Model coherence: 0.0033938294912794634\n",
      "k =  20 . Model coherence: 0.0028373181233635274\n",
      "k =  21 . Model coherence: 0.003928204220269989\n",
      "k =  22 . Model coherence: 0.002854457509624119\n",
      "k =  23 . Model coherence: 0.0002944259246887758\n",
      "k =  24 . Model coherence: 0.003591534689938406\n",
      "k =  25 . Model coherence: 0.0030409138510003684\n"
     ]
    }
   ],
   "source": [
    "max_model_coherence = 0\n",
    "res_k = 0\n",
    "\n",
    "for k in range(10,26):\n",
    "\n",
    "    nmf_model = NMF(k) \n",
    "    nmf_model.fit_transform(a)\n",
    "\n",
    "    vocabulary = [[token_list[x[1]] for x in sorted(zip(topic,range(len(topic))), reverse = True)[:t]] for topic in nmf_model.components_]\n",
    "    model = Word2Vec(sentences = vocabulary, vector_size = 200, window = 5, hs = 1, negative = 0, min_count = 1)\n",
    "    \n",
    "    # calculating individual topic coherence scores for each topic\n",
    "    model_score = []\n",
    "    for topic in vocabulary:\n",
    "        topic_score = []\n",
    "        for w1 in topic:\n",
    "            for w2 in topic:\n",
    "                if w2 > w1:\n",
    "                    word_score = cosine_similarity(model.wv[w2].reshape(1,-1),model.wv[w1].reshape(1,-1))[0]\n",
    "                    topic_score.append(word_score[0])\n",
    "        \n",
    "        topic_score = sum(topic_score)/len(topic_score) # mean of each word pair similarity in the topic\n",
    "        model_score.append(topic_score)\n",
    "\n",
    "    model_coherence = sum(model_score)/len(model_score) # mean of topic coherence in the model\n",
    "    print(\"k = \",k, \". Model coherence:\", model_coherence)\n",
    "\n",
    "    if model_coherence > max_model_coherence:\n",
    "        max_model_coherence = model_coherence\n",
    "        res_k = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(res_k) \n",
    "w = nmf_model.fit_transform(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, find the t higher weights' index and find the correpondent token (same index) in the token list. These are the descriptors of each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : ['chip', 'computer', 'science', 'silicon', 'technology', 'researcher', 'supercomputer', 'engineering', 'gene', 'intel']\n",
      "Topic 1 : ['player', 'club', 'game', 'football', 'team', 'season', 'chelsea', 'manager', 'champion', 'match']\n",
      "Topic 2 : ['party', 'election', 'tory', 'government', 'howard', 'democrat', 'policy', 'leader', 'blair', 'immigration']\n",
      "Topic 3 : ['music', 'band', 'song', 'rock', 'singer', 'single', 'album', 'artist', 'record', 'concert']\n",
      "Topic 4 : ['forsyth', 'frederick', 'terrorist', 'internment', 'forsythe', 'totalitarianism', 'qaeda', 'fundamentalism', 'churchill', 'liberty']\n",
      "Topic 5 : ['growth', 'economy', 'market', 'price', 'rate', 'rise', 'bank', 'dollar', 'analyst', 'profit']\n",
      "Topic 6 : ['angel', 'rhapsody', 'bland', 'guy', 'pulp', 'cheesy', 'brit', 'scissor', 'joke', 'listener']\n",
      "Topic 7 : ['sub', 'minute', 'goal', 'ball', 'header', 'yard', 'kick', 'cech', 'cross', 'duff']\n",
      "Topic 8 : ['virus', 'program', 'security', 'mail', 'software', 'computer', 'attack', 'user', 'firm', 'machine']\n",
      "Topic 9 : ['yukos', 'bankruptcy', 'gazprom', 'russia', 'rosneft', 'khodorkovsky', 'court', 'unit', 'yugansk', 'fraud']\n",
      "Topic 10 : ['film', 'actor', 'oscar', 'award', 'aviator', 'nomination', 'baby', 'ceremony', 'comedy', 'actress']\n",
      "Topic 11 : ['phone', 'network', 'operator', 'service', 'handset', 'speed', 'technology', 'customer', 'mobile', 'broadband']\n",
      "Topic 12 : ['blair', 'chancellor', 'peston', 'election', 'minister', 'book', 'ally', 'speculation', 'journalist', 'prescott']\n",
      "Topic 13 : ['parry', 'gerrard', 'stadium', 'xa350m', 'rafa', 'tycoon', 'sponsorship', 'club', 'deal', 'benitez']\n",
      "Topic 14 : ['gadget', 'sinclair', 'modern', 'marine', 'chronometer', 'calculator', 'apple', 'notebook', 'walkman', 'startac']\n",
      "Topic 15 : ['lord', 'court', 'law', 'case', 'right', 'trial', 'police', 'rule', 'detainee', 'lawyer']\n",
      "Topic 16 : ['search', 'site', 'information', 'google', 'engine', 'user', 'blog', 'website', 'jeeves', 'internet']\n",
      "Topic 17 : ['joss', 'stone', 'soul', 'lemar', 'jazz', 'originate', 'genre', 'meaningless', 'brit', 'therefore']\n",
      "Topic 18 : ['definition', 'dvd', 'format', 'disc', 'picture', 'hollywood', 'technology', 'film', 'studio', 'resolution']\n",
      "Topic 19 : ['game', 'developer', 'xbox', 'console', 'gamers', 'title', 'playstation', 'video', 'halo', 'graphic']\n",
      "Topic 20 : ['theatre', 'opera', 'prize', 'poppins', 'harris', 'london', 'dame', 'boy', 'book', 'lyttelton']\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(nmf_model.components_):\n",
    "    print(\"Topic\", i, \":\",[token_list[x[1]] for x in sorted(zip(topic,range(len(topic))), reverse = True)[:t]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then observe the documents with bigger weights for each topic. Because the files names already tag the contained speech by topic, we can infer the validity of the model built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : ['tech_161.txt', 'tech_173.txt', 'tech_019.txt', 'tech_329.txt', 'tech_137.txt', 'tech_141.txt', 'tech_320.txt', 'tech_166.txt', 'tech_165.txt', 'tech_018.txt']\n",
      "Topic 1 : ['football_207.txt', 'football_087.txt', 'football_088.txt', 'football_086.txt', 'football_223.txt', 'football_202.txt', 'football_085.txt', 'football_141.txt', 'football_124.txt', 'football_221.txt']\n",
      "Topic 2 : ['politics_283.txt', 'politics_213.txt', 'politics_041.txt', 'politics_220.txt', 'politics_087.txt', 'politics_122.txt', 'politics_159.txt', 'politics_272.txt', 'politics_271.txt', 'politics_275.txt']\n",
      "Topic 3 : ['entertainment_262.txt', 'entertainment_131.txt', 'entertainment_244.txt', 'entertainment_229.txt', 'entertainment_263.txt', 'entertainment_142.txt', 'entertainment_145.txt', 'entertainment_133.txt', 'entertainment_236.txt', 'entertainment_153.txt']\n",
      "Topic 4 : ['politics_290.txt', 'politics_052.txt', 'politics_160.txt', 'politics_263.txt', 'politics_189.txt', 'politics_155.txt', 'tech_245.txt', 'entertainment_057.txt', 'politics_280.txt', 'business245.txt']\n",
      "Topic 5 : ['business277.txt', 'business245.txt', 'business290.txt', 'business262.txt', 'business287.txt', 'business163.txt', 'business044.txt', 'business119.txt', 'business272.txt', 'business286.txt']\n",
      "Topic 6 : ['entertainment_253.txt', 'entertainment_251.txt', 'entertainment_201.txt', 'entertainment_128.txt', 'football_198.txt', 'entertainment_191.txt', 'entertainment_131.txt', 'entertainment_215.txt', 'entertainment_106.txt', 'entertainment_284.txt']\n",
      "Topic 7 : ['football_010.txt', 'football_253.txt', 'football_152.txt', 'football_246.txt', 'football_045.txt', 'football_174.txt', 'football_184.txt', 'football_069.txt', 'football_011.txt', 'football_012.txt']\n",
      "Topic 8 : ['tech_299.txt', 'tech_270.txt', 'tech_077.txt', 'tech_281.txt', 'tech_398.txt', 'tech_227.txt', 'tech_308.txt', 'tech_083.txt', 'tech_210.txt', 'tech_357.txt']\n",
      "Topic 9 : ['business192.txt', 'business083.txt', 'business025.txt', 'business299.txt', 'business164.txt', 'business127.txt', 'business230.txt', 'business003.txt', 'business077.txt', 'business181.txt']\n",
      "Topic 10 : ['entertainment_095.txt', 'entertainment_064.txt', 'entertainment_038.txt', 'entertainment_039.txt', 'entertainment_082.txt', 'entertainment_069.txt', 'entertainment_051.txt', 'entertainment_078.txt', 'entertainment_086.txt', 'entertainment_084.txt']\n",
      "Topic 11 : ['tech_032.txt', 'tech_335.txt', 'tech_234.txt', 'tech_389.txt', 'tech_388.txt', 'tech_103.txt', 'tech_212.txt', 'tech_225.txt', 'tech_378.txt', 'tech_349.txt']\n",
      "Topic 12 : ['politics_253.txt', 'politics_218.txt', 'politics_255.txt', 'politics_252.txt', 'politics_256.txt', 'politics_260.txt', 'politics_225.txt', 'politics_254.txt', 'politics_257.txt', 'politics_251.txt']\n",
      "Topic 13 : ['football_018.txt', 'football_027.txt', 'football_024.txt', 'business242.txt', 'football_149.txt', 'business229.txt', 'business209.txt', 'football_180.txt', 'business224.txt', 'football_157.txt']\n",
      "Topic 14 : ['tech_009.txt', 'tech_208.txt', 'tech_367.txt', 'tech_341.txt', 'tech_045.txt', 'tech_375.txt', 'tech_111.txt', 'tech_216.txt', 'tech_324.txt', 'tech_044.txt']\n",
      "Topic 15 : ['politics_224.txt', 'politics_040.txt', 'politics_221.txt', 'politics_137.txt', 'politics_168.txt', 'politics_192.txt', 'politics_160.txt', 'politics_205.txt', 'politics_229.txt', 'politics_066.txt']\n",
      "Topic 16 : ['tech_381.txt', 'tech_066.txt', 'tech_016.txt', 'tech_106.txt', 'tech_213.txt', 'tech_010.txt', 'tech_053.txt', 'tech_318.txt', 'tech_062.txt', 'tech_142.txt']\n",
      "Topic 17 : ['entertainment_256.txt', 'tech_005.txt', 'entertainment_264.txt', 'entertainment_251.txt', 'entertainment_263.txt', 'entertainment_142.txt', 'entertainment_229.txt', 'entertainment_249.txt', 'tech_310.txt', 'tech_089.txt']\n",
      "Topic 18 : ['tech_294.txt', 'tech_155.txt', 'tech_313.txt', 'tech_094.txt', 'tech_306.txt', 'tech_224.txt', 'tech_219.txt', 'tech_360.txt', 'tech_216.txt', 'tech_240.txt']\n",
      "Topic 19 : ['tech_396.txt', 'tech_135.txt', 'tech_309.txt', 'tech_139.txt', 'tech_154.txt', 'tech_125.txt', 'tech_052.txt', 'tech_178.txt', 'tech_130.txt', 'tech_244.txt']\n",
      "Topic 20 : ['entertainment_275.txt', 'entertainment_013.txt', 'entertainment_273.txt', 'entertainment_023.txt', 'entertainment_007.txt', 'entertainment_295.txt', 'entertainment_021.txt', 'entertainment_017.txt', 'entertainment_005.txt', 'entertainment_008.txt']\n"
     ]
    }
   ],
   "source": [
    "for i in range(res_k):\n",
    "    print(\"Topic\", i, \":\",[files[x[1]].split('/')[-1] for x in sorted(zip(w[:,i],range(len(w[:,i]))), reverse = True)[:t]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
